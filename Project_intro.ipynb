{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36e441eb",
   "metadata": {},
   "source": [
    "# ECBM 4040 Fall 2022 Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166344e2",
   "metadata": {},
   "source": [
    "# Project Group YAAA (ac5166, yb2540, aap2239)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654aaab",
   "metadata": {},
   "source": [
    "## Paper Title - MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da337b",
   "metadata": {},
   "source": [
    "The github repo is arranged such that each directory represents one experiment on MobileNets from the paper which was recreated by our group."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca25292",
   "metadata": {},
   "source": [
    "## Directory Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5642eda",
   "metadata": {},
   "source": [
    "### 1.) MobileNet Model Shrinking Hyperparameter\n",
    "In this experiment we try to show the effect of changing the alpha parameter in the MobileNet architecture. The alpha parameter is also called as the width multiplier which basically thins the network uniformly at each layer, creating an effectively thinner network. We trained the CIFAR-10 dataset on a custom implementation of the MobileNet architecture. The input images were of size 32x32. We trained the model for alpha in [1,0.75,0.5,0.25]. In the original paper, the authors trained their models on the ImageNet dataset with an input image size of 224x224 pixels on same alpha values. From the results we can observe that decreasing the alpha value reduced the accuracy of the model but at the same time decreased the amount of time taken to train each epoch which is consistent with the observations from the paper. \n",
    "\n",
    "Files included:\n",
    "\n",
    "Modeling.ipynb - notebook that includes end to end code for this experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0216bdb2",
   "metadata": {},
   "source": [
    "### 2.) Fine-Grained Recognition\n",
    "Aim of this experiment is to evaluate the performance of MobileNet on a noisy dataset that requires fine-grained recognition capabilities. We downloaded the Stanford Dogs dataset as done in the experiment. We subsampled 40 classes of dog images from the overall 120 to respect computational constraints. We additionally created synthetic data by techniques such as rescaling, shearing, zooming, flipping, and rotating. The mobilenetv2 model Keras API was called and an additional global average pooling layer and a SoftMax dense output layer with 40 neurons were added. The model was trained on the data for different width multiplier alpha and resolution values as done in the original paper. \n",
    "\n",
    "Files included:\n",
    "\n",
    "Modeling.ipynb - notebook that includes end to end code for this experiment. \n",
    "\n",
    "The dataset is too large to be included in github but can be found here (http://vision.stanford.edu/aditya86/ImageNetDogs/). We work with a subsample of this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c318a8",
   "metadata": {},
   "source": [
    "### 3.) Face Attributes\n",
    "\n",
    "Aim of this experiment is to use MobileNet to efficiently identify the defining facial attributes of peopleâ€™s faces. Due to computational constraints, to recreate this experiment instead of YFCC100M dataset we have used similar 5000 images from the CelebFaces Attributes (CelebA) Dataset from Kaggle. This dataset has the image of celebrity faces as input and a 10-feature output dataframe which has 10 face markers used to map the facial attributes of the person such as the x,y coordinates of both eys, ears, nose and mouth ends. After preprocessing steps such as normalization and rescaling, we used transfer learning. We called the mobilenet model Keras API and added a global average pooling layer and a relu-based dense output layer with 10 neurons i.e. our 10 face attribute markers. On training, we obtain a mean absolute percentage error of 4.2%, i.e. accuracy of nearly 96%.\n",
    "\n",
    "Files included:\n",
    "\n",
    "Modeling.ipynb - notebook that includes end to end code for this experiment.\n",
    "\n",
    "The dataset is too large to be included in github but can be found here (https://www.kaggle.com/datasets/jessicali9530/celeba-dataset). We work with a subsample of this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53116551",
   "metadata": {},
   "source": [
    "### 4.) Object Detection\n",
    "\n",
    "Aim of this experiment is to train the MobileNet on the COCO dataset for the object detection task. Due to computational constraints, we use 5000 images from the COCO dataset. This dataset as input has a variety of images where each image can have multiple distinct objects. The output is a 4-feature output dataframe where each row represents the (xmin,xmax,ymin,ymax) for an object in each image i.e. bouding box coordinates for the object. We use transfer learning. We called the mobilenet model Keras API and added a global average pooling layer and a relu-based dense output layer with 4 neurons i.e. our 4 bounding box coordinates. On training, we obtain a root mean squared error (RMSE) of 122.82.\n",
    "\n",
    "Files included:\n",
    "\n",
    "Modeling.ipynb - notebook that includes end to end code for this experiment\n",
    "\n",
    "coco_minitrain2017.csv - output labels for the COCO dataset\n",
    "\n",
    "The dataset is too large to be included in github but can be found here (https://drive.google.com/file/d/1t_l9uyBPfxSEzcajTk4a1TaQXzeRm9hw/view). We work with a subsample of this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1feb6660",
   "metadata": {},
   "source": [
    "### 5.) Distillation based training\n",
    "\n",
    "Aim of this experiment is to implement knowledge distillation i.e. process of transferring knowledge from a large model to a smaller one. To implement this, we use MobileNet as our teacher architecture and a simple 5-layer CNN as the student architecture upon which MobileNet will distill its learning. We use MNIST dataset for our purposes. We train the MobileNet model on the dataset followed by distillation or transfer of the learning by the teacher architecture upon the simpler student CNN model. Using this method, we can see our student architecture yield an accuracy of 96.4%. To check the efficacy of this method we also trained the student CNN architecture directly on the dataset and we obtain an accuracy of 97.67%. Since the results from both methods are comparable, we can conclude knowledge distillation is highly effective.\n",
    "\n",
    "Files included:\n",
    "\n",
    "distiller_trainer.py - code for custom distillation based training logic from teacher architecture to student architecture\n",
    "\n",
    "Modeling.ipynb - notebook that includes end to end code for this experiment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
